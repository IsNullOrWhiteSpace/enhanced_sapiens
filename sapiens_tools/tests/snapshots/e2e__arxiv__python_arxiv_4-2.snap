
---
source: sapiens_tools/tests/arxiv.rs
expression: termination_messages
---
- conclusion: "Title: The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules\n  of Thumb\nSummary:   Simple economic and performance arguments suggest appropriate lifetimes for\nmain memory pages and suggest optimal page sizes. The fundamental tradeoffs are\nthe prices and bandwidths of RAMs and disks. The analysis indicates that with\ntoday's technology, five minutes is a good lifetime for randomly accessed\npages, one minute is a good lifetime for two-pass sequentially accessed pages,\nand 16 KB is a good size for index pages. These rules-of-thumb change in\npredictable ways as technology ratios change. They also motivate the importance\nof the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.\n\nPDF Url: https://arxiv.org/pdf/cs/9809005v1.pdf\n\n\nTitle: Similarity-Based Queries for Time Series Data\nSummary:   We study a set of linear transformations on the Fourier series representation\nof a sequence that can be used as the basis for similarity queries on\ntime-series data. We show that our set of transformations is rich enough to\nformulate operations such as moving average and time warping. We present a\nquery processing algorithm that uses the underlying R-tree index of a\nmultidimensional data set to answer similarity queries efficiently. Our\nexperiments show that the performance of this algorithm is competitive to that\nof processing ordinary (exact match) queries using the index, and much faster\nthan sequential scanning. We relate our transformations to the general\nframework for similarity queries of Jagadish et al.\n\nPDF Url: https://arxiv.org/pdf/cs/9809023v2.pdf\n\n\nTitle: Efficient Retrieval of Similar Time Sequences Using DFT\nSummary:   We propose an improvement of the known DFT-based indexing technique for fast\nretrieval of similar time sequences. We use the last few Fourier coefficients\nin the distance computation without storing them in the index since every\ncoefficient at the end is the complex conjugate of a coefficient at the\nbeginning and as strong as its counterpart. We show analytically that this\nobservation can accelerate the search time of the index by more than a factor\nof two. This result was confirmed by our experiments, which were carried out on\nreal stock prices and synthetic data.\n\nPDF Url: https://arxiv.org/pdf/cs/9809033v2.pdf\n\n\nTitle: Least expected cost query optimization: an exercise in utility\nSummary:   We identify two unreasonable, though standard, assumptions made by database\nquery optimizers that can adversely affect the quality of the chosen evaluation\nplans. One assumption is that it is enough to optimize for the expected\ncase---that is, the case where various parameters (like available memory) take\non their expected value. The other assumption is that the parameters are\nconstant throughout the execution of the query. We present an algorithm based\non the ``System R''-style query optimization algorithm that does not rely on\nthese assumptions. The algorithm we present chooses the plan of the least\nexpected cost instead of the plan of least cost given some fixed value of the\nparameters. In execution environments that exhibit a high degree of\nvariability, our techniques should result in better performance.\n\nPDF Url: https://arxiv.org/pdf/cs/9909016v1.pdf"
  original_question: What are the 4 latest papers on database published on arXiv? What are they about?
